{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm\n",
    "\n",
    "Based on this example: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "\n",
    "1. Set gamma and matrix R\n",
    "2. Initialize Q to zero\n",
    "3. For each episode (training section):\n",
    "    - select a random init state\n",
    "    - While goal has not been reached\n",
    "        - Select one among all possible actions for the current state\n",
    "        - Using this possible action, consider going to the next state\n",
    "        - Get maxQ for this next state based on all possible actions\n",
    "        - Compute $Q(state, nextstate) = R(state, next state) + gamma*max[Q(next state, all third states)]$\n",
    "    - end while\n",
    "4. End for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set gamma and matrix R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#                0   1   2   3   4   5\n",
    "R = np.matrix([[-1, -1, -1, -1,  0,  -1],   # 0 \n",
    "               [-1, -1, -1,  0, -1, 100],   # 1\n",
    "               [-1, -1, -1,  0, -1,  -1],   # 2\n",
    "               [-1,  0,  0, -1,  0,  -1],   # 3\n",
    "               [ 0, -1, -1,  0, -1, 100],   # 4\n",
    "               [-1,  0, -1, -1,  0, 100]])  # 5\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "target_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Q to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = np.zeros((6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. For each episode (training section - Step by step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Select a random init state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "random_init_state = np.random.randint(6)\n",
    "print random_init_state\n",
    "state = random_init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. While goal has not been reached (step by step)\n",
    "```\n",
    "#                0   1   2   3   4   5\n",
    "R = np.matrix([[-1, -1, -1, -1,  0,  -1],   # 0 \n",
    "               [-1, -1, -1,  0, -1, 100],   # 1\n",
    "               [-1, -1, -1,  0, -1,  -1],   # 2\n",
    "               [-1,  0,  0, -1,  0,  -1],   # 3\n",
    "               [ 0, -1, -1,  0, -1, 100],   # 4\n",
    "               [-1,  0, -1, -1,  0, 100]])  # 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Select one among all possible actions (next state) for the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Current state\n",
    "print state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 5]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all possible next states options\n",
    "next_state_options = [ns for ns,r in enumerate(np.nditer(R[state])) if r>=0]\n",
    "next_state_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a random choice of next state\n",
    "next_state = random.choice(next_state_options)\n",
    "next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Compute $Q(state, nextstate) = R(state, next state) + gamma*max[Q(next state, all third states)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.8\n",
      "[  0.   0.   0.   0.  80.   0.]\n",
      "80.0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print Q[state, next_state]\n",
    "print R[state, next_state]\n",
    "print gamma\n",
    "print Q[next_state]\n",
    "print max(Q[next_state])\n",
    "print Q[next_state].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [  64.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "state = next_state\n",
    "print state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Check if goal has been reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal has not been reached\n"
     ]
    }
   ],
   "source": [
    "if state == target_state:\n",
    "    print 'goal has been reached'\n",
    "else:\n",
    "    print 'goal has not been reached'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. While goal has not been reached (straight forward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "new random init state\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print reward matrix\n",
    "print 'Reward Matrix'\n",
    "print R\n",
    "print '\\n'\n",
    "\n",
    "# Current state\n",
    "print 'Current state = {}'.format(state)\n",
    "print '\\n'\n",
    "\n",
    "# Check all possible next states options\n",
    "next_state_options = [ns for ns,r in enumerate(np.nditer(R[state])) if r>=0]\n",
    "print 'Possible next states = {}'.format(next_state_options)\n",
    "print '\\n'\n",
    "\n",
    "# Make a random choice of next state\n",
    "next_state = random.choice(next_state_options)\n",
    "print 'Random choice of next state = {}'.format(next_state)\n",
    "print '\\n'\n",
    "\n",
    "# Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
    "print 'Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])'\n",
    "print 'Q[state, next_state] = {}'.format(Q[state, next_state])\n",
    "print 'R[state, next_state] = {}'.format(R[state, next_state])\n",
    "print 'gamma = {}'.format(gamma)\n",
    "print 'Q[next_state] = {}'.format(Q[next_state])\n",
    "print 'max(Q[next_state]) = {}'.format(max(Q[next_state]))\n",
    "print 'Q[next_state].argmax() = {}'.format(Q[next_state].argmax())\n",
    "print '\\n'\n",
    "\n",
    "Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
    "print 'Q Matrix'\n",
    "print Q\n",
    "print '\\n'\n",
    "\n",
    "# Set next state as new state\n",
    "print 'Action: move from state {} to {}'.format(state, next_state)\n",
    "state = next_state\n",
    "print '\\n'\n",
    "\n",
    "# Check if goal has been reached\n",
    "print 'Checking if goal has been reached:'\n",
    "if state == target_state:\n",
    "    print 'goal has been reached, lets start a new episode'\n",
    "    random_init_state = np.random.randint(6)\n",
    "    print 'new random init state = {}'.format(random_init_state)\n",
    "    state = random_init_state\n",
    "    print '\\n'\n",
    "else:\n",
    "    print 'goal has not been reached'\n",
    "print '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modeling_environment_clip_image002a.gif](http://mnemstudio.org/ai/path/images/modeling_environment_clip_image002a.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. For each episode (training section - Straight forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training section number 1 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 2\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 2\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 2\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 2\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 2 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 3 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.    0.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 4 ###\n",
      "new random init state = 1\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 5 ###\n",
      "new random init state = 0\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.    0.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.   0.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.    0.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.    0.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 6 ###\n",
      "new random init state = 5\n",
      "\n",
      "\n",
      "### Training section number 7 ###\n",
      "new random init state = 1\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.   0.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 8 ###\n",
      "new random init state = 1\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 9 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 2\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 2\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.   64.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.   64.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.   64.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.   64.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.  80.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.    0.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.   64.    0.    0.]\n",
      " [   0.   80.    0.    0.   80.    0.]\n",
      " [   0.    0.    0.   64.    0.  100.]\n",
      " [   0.    0.    0.    0.    0.    0.]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 2\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.  64.   0.   0.]\n",
      "max(Q[next_state]) = 64.0\n",
      "Q[next_state].argmax() = 3\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 2\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 10 ###\n",
      "new random init state = 1\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 0.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 11 ###\n",
      "new random init state = 5\n",
      "\n",
      "\n",
      "### Training section number 12 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 13 ###\n",
      "new random init state = 4\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 14 ###\n",
      "new random init state = 5\n",
      "\n",
      "\n",
      "### Training section number 15 ###\n",
      "new random init state = 5\n",
      "\n",
      "\n",
      "### Training section number 16 ###\n",
      "new random init state = 3\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 17 ###\n",
      "new random init state = 4\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 18 ###\n",
      "new random init state = 2\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 19 ###\n",
      "new random init state = 2\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 0\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.   0.  80.   0.]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 4\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 0\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 0\n",
      "\n",
      "\n",
      "Possible next states = [4]\n",
      "\n",
      "\n",
      "Random choice of next state = 4\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  64.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 0 to 4\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 4\n",
      "\n",
      "\n",
      "Possible next states = [0, 3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 4 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 2\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 51.2\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   0.   0.  64.   0.   0.]\n",
      "max(Q[next_state]) = 64.0\n",
      "Q[next_state].argmax() = 3\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 2\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 2\n",
      "\n",
      "\n",
      "Possible next states = [3]\n",
      "\n",
      "\n",
      "Random choice of next state = 3\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 64.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [  0.   80.   51.2   0.   80.    0. ]\n",
      "max(Q[next_state]) = 80.0\n",
      "Q[next_state].argmax() = 1\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 2 to 3\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 3\n",
      "\n",
      "\n",
      "Possible next states = [1, 2, 4]\n",
      "\n",
      "\n",
      "Random choice of next state = 1\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 80.0\n",
      "R[state, next_state] = 0\n",
      "gamma = 0.8\n",
      "Q[next_state] = [   0.    0.    0.   64.    0.  100.]\n",
      "max(Q[next_state]) = 100.0\n",
      "Q[next_state].argmax() = 5\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 3 to 1\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has not been reached\n",
      "\n",
      "\n",
      "Reward Matrix\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "\n",
      "Current state = 1\n",
      "\n",
      "\n",
      "Possible next states = [3, 5]\n",
      "\n",
      "\n",
      "Random choice of next state = 5\n",
      "\n",
      "\n",
      "Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
      "Q[state, next_state] = 100.0\n",
      "R[state, next_state] = 100\n",
      "gamma = 0.8\n",
      "Q[next_state] = [ 0.  0.  0.  0.  0.  0.]\n",
      "max(Q[next_state]) = 0.0\n",
      "Q[next_state].argmax() = 0\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[   0.     0.     0.     0.    80.     0. ]\n",
      " [   0.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.    64.     0.     0. ]\n",
      " [   0.    80.    51.2    0.    80.     0. ]\n",
      " [  64.     0.     0.    64.     0.   100. ]\n",
      " [   0.     0.     0.     0.     0.     0. ]]\n",
      "\n",
      "\n",
      "Action: move from state 1 to 5\n",
      "\n",
      "\n",
      "Checking if goal has been reached:\n",
      "goal has been reached, lets start a new episode\n",
      "\n",
      "\n",
      "### Training section number 20 ###\n",
      "new random init state = 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_episodes = 20\n",
    "for i in range(number_episodes):\n",
    "    print '### Training section number {} ###'.format(i+1)\n",
    "    random_init_state = np.random.randint(6)\n",
    "    print 'new random init state = {}'.format(random_init_state)\n",
    "    state = random_init_state\n",
    "    print '\\n'\n",
    "    \n",
    "    while state != target_state:\n",
    "        # Print reward matrix\n",
    "        print 'Reward Matrix'\n",
    "        print R\n",
    "        print '\\n'\n",
    "\n",
    "        # Current state\n",
    "        print 'Current state = {}'.format(state)\n",
    "        print '\\n'\n",
    "\n",
    "        # Check all possible next states options\n",
    "        next_state_options = [ns for ns,r in enumerate(np.nditer(R[state])) if r>=0]\n",
    "        print 'Possible next states = {}'.format(next_state_options)\n",
    "        print '\\n'\n",
    "\n",
    "        # Make a random choice of next state\n",
    "        next_state = random.choice(next_state_options)\n",
    "        print 'Random choice of next state = {}'.format(next_state)\n",
    "        print '\\n'\n",
    "\n",
    "        # Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
    "        print 'Compute Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])'\n",
    "        print 'Q[state, next_state] = {}'.format(Q[state, next_state])\n",
    "        print 'R[state, next_state] = {}'.format(R[state, next_state])\n",
    "        print 'gamma = {}'.format(gamma)\n",
    "        print 'Q[next_state] = {}'.format(Q[next_state])\n",
    "        print 'max(Q[next_state]) = {}'.format(max(Q[next_state]))\n",
    "        print 'Q[next_state].argmax() = {}'.format(Q[next_state].argmax())\n",
    "        print '\\n'\n",
    "\n",
    "        Q[state, next_state] = R[state, next_state] + gamma*max(Q[next_state])\n",
    "        print 'Q Matrix'\n",
    "        print Q\n",
    "        print '\\n'\n",
    "\n",
    "        # Set next state as new state\n",
    "        print 'Action: move from state {} to {}'.format(state, next_state)\n",
    "        state = next_state\n",
    "        print '\\n'\n",
    "\n",
    "        # Check if goal has been reached\n",
    "        print 'Checking if goal has been reached:'\n",
    "        if state == target_state:\n",
    "            print 'goal has been reached, lets start a new episode'\n",
    "        else:\n",
    "            print 'goal has not been reached'\n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
